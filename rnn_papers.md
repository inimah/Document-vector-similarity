Reading list ( RNN literatures )

**1. Comparison between variants of RNN**

| Reference        |Summary | 
| ------------- | ------------- | 
| \[1]. (Chung et. al, 2016)      |  | 
| \[2]. (Zaremba, 2015)     |  | 
| \[3]. (Greff, 2016)     |  | 



\[1]. Chung, Junyoung, et al. "Empirical evaluation of gated recurrent neural networks on sequence modeling." arXiv preprint arXiv:1412.3555 (2014).

\[2]. Zaremba, Wojciech. "An empirical exploration of recurrent network architectures." (2015).

\[3]. Greff, Klaus, et al. "LSTM: A search space odyssey." IEEE transactions on neural networks and learning systems (2016).


-----------------------------------------------------------------------------

**2. Variants of RNN and LSTM**

| Reference        |Summary | 
| ------------- | ------------- | 
| \[1]. (Cho, 2014)      | GRU | 
| \[2]. (Zaremba, 2015)     | Depth RNN | 
| \[3]. (Koutnik, 2014)     | Clockwork RNN | 
| \[4]. (Kalchbrenner, 2015)     | Grid LSTM | 



\[1]. Cho, Kyunghyun, et al. "Learning phrase representations using RNN encoder-decoder for statistical machine translation." arXiv preprint arXiv:1406.1078 (2014).

\[2]. Yao, Kaisheng, et al. "Depth-Gated Recurrent Neural Networks." arXiv preprint arXiv:1508.03790 (2015).

\[3]. Koutnik, Jan, et al. "A clockwork rnn." arXiv preprint arXiv:1402.3511 (2014).

\[4]. Kalchbrenner, Nal, Ivo Danihelka, and Alex Graves. "Grid long short-term memory." arXiv preprint arXiv:1507.01526 (2015).


